{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、项目背景\n",
    "\n",
    "## 1.1 案例研究\n",
    "\n",
    "【案例研究】金融机构信用卡申请欺诈识别实践——基于\"北X公寓\"团伙攻击事件分析\n",
    "\n",
    "在信用卡审批流程中，风险管理部门通过异常地址监测发现重大团伙欺诈线索：短期内集中涌现大量户籍地为A省B市的信用卡申请件，其邮寄地址高度统一指向\"C市E区北三环xxx路xx号x楼北（晨/辰/宸）公寓\"的特殊地址集群。经实地核验及交易回溯，确认该批申请系黑中介团伙以\"免费办理POS机/收款二维码\"为诱饵，非法收集客户身份信息后进行资质包装，通过代办信用卡申请并收取高额手续费实施欺诈的典型案件。\n",
    "\n",
    "本批次案件呈现显著团伙作案特征：经系统筛查，邮寄地址字段包含\"北晨/辰公寓\"的申请件共计598件，涉及538名申请人。剔除节假日促销等特殊场景进件后，该地址日常申请量趋近于零，而本次攻击集中爆发于27-29日三天内，时间密度异常突出。申请人资质呈现系统性偏差——本科及以上学历占比仅3.18%，远低于正常客群水平；信用评分分布显著左偏，整体资质明显劣于常规进件。\n",
    "\n",
    "值得关注的是，该团伙虽通过变换IP地址、手机号等基础伪装手段试图规避检测，但仍能通过多维关联规则引擎成功拦截多数风险申请。系统监测显示，涉案申请件的IP地址聚类系数、手机号关联度等指标呈现强相关性，触发\"设备指纹异常\"\"地址集群风险\"等策略规则，形成有效防御屏障。\n",
    "\n",
    "本案深刻揭示：欺诈与反欺诈是永不停歇的攻防博弈。当前黑产已形成\"信息收集-资质伪造-批量申请-手续费分成\"的完整产业链，攻击手段持续迭代升级。传统基于单一维度（如IP/手机号切换）的防御策略已难以应对新型团伙攻击，亟需构建多维智能识别体系。\n",
    "\n",
    "此时，升级自然语言处理引擎，精准识别地址字段中的非常规字符变异（如\"晨/辰/宸\"同音字替换），建立地址热力图动态监测系统，对非常规地址集群实施秒级预警，强化风险识别能力，构建\"数据驱动+智能算法+持续迭代\"的立体化防控体系，方能在动态博弈中保持反欺诈防御优势。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 任务说明\n",
    "\n",
    "在收集示例数据之后，我们定义了以下地址匹配任务：\n",
    "\n",
    "聚焦于地址文本的相关性评估：给定一条地址查询query与若干候选地址，模型需对二者的匹配程度进行打分。\n",
    "\n",
    "* 输入：输入文件包含若干个query-地址文本对；\n",
    "* 输出：输出文本每一行包括此query-地址文本对的匹配程度，分为完全匹配、部分匹配、不匹配；\n",
    "\n",
    "示例：\n",
    "```json\n",
    "输入：\n",
    "{\n",
    "    \"query\": \"江苏省南京市清水亭东路9号金域蓝湾15幢\", \n",
    "    \"candidate\": [\n",
    "        {\n",
    "            \"text\": \"江宁区万科金域蓝湾15栋\"\n",
    "        }, \n",
    "        {\n",
    "            \"text\": \"江苏省南京市清水亭东路9号\"\n",
    "        }, \n",
    "        {\n",
    "            \"text\": \"新水泥路666号重工数控工业园\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "输出：\n",
    "{\n",
    "    \"query\": \"江苏省南京市清水亭东路9号金域蓝湾15幢\", \n",
    "    \"candidate\": [\n",
    "        {\n",
    "            \"text\": \"江宁区万科金域蓝湾15栋\", \n",
    "            \"label\": \"完全匹配\"\n",
    "        }, \n",
    "        {\n",
    "            \"text\": \"江苏省南京市清水亭东路9号\", \n",
    "            \"label\": \"部分匹配\"\n",
    "        }, \n",
    "        {\n",
    "            \"text\": \"新水泥路666号重工数控工业园\", \n",
    "            \"label\": \"不匹配\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "数据上，随机切分为训练集和测试集，可分情况选用训练集。\n",
    "\n",
    "其中测试集的标签不可见，最终将在测试集上完成黑盒评测，以各个匹配程度标签F1平均值为目标分数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 传统算法\n",
    "\n",
    "上述地址匹配任务面临的三大挑战如下：\n",
    "\n",
    "* 写法多样：同一实体地址存在大量异构表达，且无官方改写词表可供查照。\n",
    "* 语境约束：query 常携带省、市、区等限定信息，模型需据此精准判断候选地址的匹配与否。\n",
    "* 规范差异：各地地址格式与习惯差异显著，对模型的跨域泛化能力提出了更高要求。\n",
    "\n",
    "这些对于金融机构以地址库和规则匹配为核心的传统算法，有一定的难度。\n",
    "\n",
    "以下，我们介绍传统算法其中之二（模型A、模型B），并在后续将它们作为准绳：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 模型A\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"https://ai-studio-static-online.cdn.bcebos.com/ad32f2cfe905477db9e34fc15b5765b49b042189cf364e84810ed0bd78397ead\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "基于某S数据服务厂商提供的地址匹配技术，需依赖内置的离线Quality Knowledge Base知识库K的Address模块，可理解为核心地址库+正则表达式库，再对地址进行标准化和Match code转换，转换后核心匹配逻辑类似于相似度计算。\n",
    "\n",
    "### 1.3.2 模型B\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"https://ai-studio-static-online.cdn.bcebos.com/54f5b51bc7a5479d88eeff4e00937e19b29c463ba71c4aaf85264a33cbf33225\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "某X金融科技自研的工业化算法，该模型基于模型A开发，进一步强化标准地址库，复合更细的专家匹配规则，融合多维度相似度，采用动态更新和增量训练机制，持续提升模型泛化能力。\n",
    "\n",
    "### 1.3.3 模型C\n",
    "\n",
    "这是一个简单的模型融合，用于模型对比，对于模型A、模型B各取所长。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、文心大模型\n",
    "\n",
    "特别说明，由于传统算法有比较复杂的地址库和规则引擎，不一定具备直接进行模型细调/Fine-Tuning的能力。\n",
    "\n",
    "我们在此区分两种场景进行对比：\n",
    "\n",
    "**(1) 泛化能力**\n",
    "\n",
    "仅在测试集上，调用模型A、模型B、文心ERNIE系列模型，直接进行黑盒评测，对比其F1分数。\n",
    "\n",
    "**(2) 模型细调**\n",
    "\n",
    "引入训练集，基于文心ERNIE系列模型进行SFT细调，随后在测试集上进行黑盒评测，对比其F1分数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 泛化能力\n",
    "\n",
    "### 2.1.1 快速部署\n",
    "\n",
    "一键加载，自动下载，速度400M/s+，优秀到起飞！\n",
    "\n",
    "注意：需要Tesla A800 80GB，实际显存占用50%～95%\n",
    "\n",
    "* 0.server.sh\n",
    "\n",
    "```shell\n",
    "pip install pandarallel\n",
    "\n",
    "clear\n",
    "path=/home/aistudio/data/models\n",
    "model=ERNIE-4.5-21B-A3B-Base-Paddle\n",
    "\n",
    "echo $model\n",
    "rm -rf $path/$model\n",
    "aistudio download --model PaddlePaddle/$model --local_dir $path/$model\n",
    "ls -l $path/$model\n",
    "\n",
    "python -m fastdeploy.entrypoints.openai.api_server --model $path/$model --port 8180 --metrics-port 8181 --engine-worker-queue-port 8182 --max-model-len 1024 --max-num-seqs 128\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Prompt工程\n",
    "\n",
    "这里推荐**模型体验场**[https://aistudio.baidu.com/playground](https://aistudio.baidu.com/playground)\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/8ae8fb1392d84633979b6fa34aa118580e33f68f916d4911969eb930067d7ab9)\n",
    "\n",
    "可进行模型配置调试后，生成并复制相关代码。\n",
    "\n",
    "* 1.test.py\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "# Import\n",
    "from pandarallel import pandarallel\n",
    "# Initialization\n",
    "pandarallel.initialize(nb_workers=64, progress_bar=True)\n",
    "\n",
    "try:\n",
    "    work_txt = sys.argv[1]\n",
    "except:\n",
    "    work_txt = \"x3nlp_1_test.txt\"\n",
    "# os.system(\"clear\")\n",
    "print(f\"work_txt: {work_txt}\")\n",
    "\n",
    "host = \"0.0.0.0\"\n",
    "port = \"8180\"\n",
    "client = openai.Client(base_url=f\"http://{host}:{port}/v1\", api_key=\"null\")\n",
    "\n",
    "_system = {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": \"\"\"\n",
    "你是一个地址匹配专家，你需要判断输入的json中，query和candidate下的每一个text都是否为同一个地址信息，如果匹配正确则label为\"完全匹配\"，如果匹配不正确则label为\"不匹配\"，如果只有部分匹配则label为\"部分匹配\"，返回结果严格按照json格式。\n",
    "\n",
    "输入：\n",
    "{\"query\": \"江苏省南京市清水亭东路9号金域蓝湾15幢\", \"candidate\": [{\"text\": \"江宁区万科金域蓝湾15栋\"}, {\"text\": \"江苏省南京市清水亭东路9号\"}, {\"text\": \"新水泥路666号重工数控工业园\"}]}\n",
    "\n",
    "输出：\n",
    "{\"query\": \"江苏省南京市清水亭东路9号金域蓝湾15幢\", \"candidate\": [{\"text\": \"江宁区万科金域蓝湾15栋\", \"label\": \"完全匹配\"}, {\"text\": \"江苏省南京市清水亭东路9号\", \"label\": \"部分匹配\"}, {\"text\": \"新水泥路666号重工数控工业园\", \"label\": \"不匹配\"}]}\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "def get(_c, _i=\"\"):\n",
    "    _jc = json.loads(_c)\n",
    "    _jc_id = _jc[\"text_id\"]\n",
    "    _c = json.dumps({\n",
    "        \"query\": _jc[\"query\"],\n",
    "        \"candidate\": [{\"text\": i[\"text\"]} for i in _jc[\"candidate\"]],\n",
    "    }, ensure_ascii=False)\n",
    "    # print(_c)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"null\",\n",
    "        messages=[_system, {\"role\": \"user\", \"content\": _c}],\n",
    "        stream=False,\n",
    "\n",
    "        extra_body={\n",
    "            \"penalty_score\": 0.0,   # 关闭惩罚，避免干扰匹配\n",
    "        },\n",
    "        max_completion_tokens=512,  # 地址通常较短，\n",
    "        temperature=0.01,           # 完全确定性输出，避免随机性\n",
    "        top_p=1.0,                  # 关闭核采样（与temperature=0冲突，可保留）\n",
    "        frequency_penalty=0.0,      # 不惩罚重复（地址可能有重复元素）\n",
    "        presence_penalty=0.0,       # 不惩罚存在性（关键信息需保留）\n",
    "\n",
    "        response_format={\n",
    "            \"type\": \"json_object\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\"},\n",
    "                    \"candidate\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"text\": {\"type\": \"string\"},\n",
    "                                \"label\": {\"type\": \"string\", \"enum\": [\"完全匹配\", \"部分匹配\", \"不匹配\"]}\n",
    "                            },\n",
    "                            \"required\": [\"text\", \"label\"]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\", \"candidate\"]\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        _j = json.loads(\n",
    "            response.choices[0].message.content.replace(\"`\",\"\").replace(\"json\",\"\")\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # print(f\"\\n \\033[1;36m ERROR {_i}:\\033[0m \\n{response.choices[0].message.content[:100]}\\n{e}\", )\n",
    "        _j = json.loads(_c)\n",
    "    finally:\n",
    "        _j[\"text_id\"] = _jc_id\n",
    "\n",
    "        for _ji in _j[\"candidate\"]:\n",
    "            if \"label\" not in _ji:\n",
    "                _ji.update({\"label\": \"?匹配?\"})\n",
    "        return json.dumps(_j, ensure_ascii=False)\n",
    "\n",
    "\n",
    "t = \"\"\"\n",
    "{\"text_id\": \"2b51366fdd6c620a3f54b520a8ebc5e5\", \"query\": \"兴东街道铁成佳园农贸大市场2厅\", \"candidate\": [{\"text\": \"沙河镇街9-1号铁成佳园农贸大市场\", \"label\": \"完全匹配\"}, {\"text\": \"沙河镇街11号铁成佳园农贸大市场(东南1门)\", \"label\": \"不匹配\"}, {\"text\": \"宏安路与沙河镇街交叉口北50米铁成佳园农贸大市场(东南2门)\", \"label\": \"部分匹配\"}, {\"text\": \"37县道佳园农场\", \"label\": \"部分匹配\"}, {\"text\": \"杭州大道农贸大市场\", \"label\": \"不匹配\"}]}\n",
    "\"\"\"\n",
    "print(f\"Test:\\n{t}\\n{get(t)}\\nStart:\\n\")\n",
    "# raise \"Test\"\n",
    "\n",
    "data = pd.read_csv(\n",
    "    work_txt, sep=\"\\t\", header=None, names=[\"text\"]\n",
    ")\n",
    "# parallel_apply\n",
    "data[\"result\"] = data[\"text\"].parallel_apply(get)\n",
    "\n",
    "with open(\"test.txt\", \"w\") as f:\n",
    "    for i in data[\"result\"]:\n",
    "        f.write(f\"{i}\\n\")\n",
    "\n",
    "```\n",
    "\n",
    "其中，模型配置上，考虑地址匹配场景，使用了以下设置：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extra_body={\n",
    "    \"penalty_score\": 0.0,   # 关闭惩罚，避免干扰匹配\n",
    "},\n",
    "max_completion_tokens=512,  # 地址通常较短，\n",
    "temperature=0.01,           # 完全确定性输出，避免随机性\n",
    "top_p=1.0,                  # 关闭核采样（与temperature=0冲突，可保留）\n",
    "frequency_penalty=0.0,      # 不惩罚重复（地址可能有重复元素）\n",
    "presence_penalty=0.0,       # 不惩罚存在性（关键信息需保留）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回的json问题，也通过response_format定义清楚，这要比在content中限制更为明确一些："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_format={\n",
    "    \"type\": \"json_object\",\n",
    "    \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\"type\": \"string\"},\n",
    "            \"candidate\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"text\": {\"type\": \"string\"},\n",
    "                        \"label\": {\"type\": \"string\", \"enum\": [\"完全匹配\", \"部分匹配\", \"不匹配\"]}\n",
    "                    },\n",
    "                    \"required\": [\"text\", \"label\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\", \"candidate\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特别的，为了响应并发调用，这里使用了parallel_apply调用，效果还可以，仅供参考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import\n",
    "from pandarallel import pandarallel\n",
    "# Initialization\n",
    "pandarallel.initialize(nb_workers=64, progress_bar=True)\n",
    "\n",
    "data = pd.read_csv(\n",
    "    work_txt, sep=\"\\t\", header=None, names=[\"text\"]\n",
    ")\n",
    "# parallel_apply\n",
    "data[\"result\"] = data[\"text\"].parallel_apply(get)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 格式优化\n",
    "\n",
    "由于json输出是我们的本质要求，也进行了特别说明和提示词定义，但为保证评测准确，我们对输出结果进行格式化：\n",
    "\n",
    "* 2.format.py\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    work_txt = sys.argv[1]\n",
    "except:\n",
    "    work_txt = \"x3nlp_1_test.txt\"\n",
    "# os.system(\"clear\")\n",
    "print(f\"work_txt: {work_txt}\")\n",
    "\n",
    "# \n",
    "# \n",
    "\n",
    "def lookup(k, l, _k):\n",
    "    for i in l:\n",
    "        if i[\"text\"] == k:\n",
    "            return {\n",
    "                \"完全匹配\": \"完全匹配\",\n",
    "                \"部分匹配\": \"部分匹配\",\n",
    "                \"不匹配\": \"不匹配\",\n",
    "\n",
    "                \"?匹配?\": \"不匹配\",\n",
    "            }.get(i[\"label\"], \"不匹配\")\n",
    "    print(\"X1\")\n",
    "    return \"部分匹配\"\n",
    "\n",
    "w = open(\"test_format.txt\", \"w\")\n",
    "with open(work_txt, \"r\") as f:\n",
    "    for j0 in tqdm(f, total=15000):\n",
    "        j0 = json.loads(j0)\n",
    "\n",
    "        try:\n",
    "            j1 = os.popen(f\"cat test.txt|grep {j0['text_id']}\").readlines()[0]\n",
    "        except:\n",
    "            j1 = '{\"candidate\": []}'\n",
    "            print(\"X2\")\n",
    "        j1 = json.loads(j1)\n",
    "\n",
    "        # format\n",
    "        j2 = {\n",
    "            \"text_id\": j0[\"text_id\"],\n",
    "            \"query\": j0[\"query\"],\n",
    "            \"candidate\": [\n",
    "                {\n",
    "                    \"text\": i[\"text\"],\n",
    "                    \"label\": lookup(i[\"text\"], j1[\"candidate\"], j0[\"query\"]),\n",
    "                }\n",
    "                for i in j0[\"candidate\"]\n",
    "            ]\n",
    "        }\n",
    "        w.write(f\"{json.dumps(j2, ensure_ascii=False)}\\n\")\n",
    "        # print(f\"{'-'*100}\\nj0: {j0}\\nj1: {j1}\\nj2: {j2}\\n{'-'*100}\")\n",
    "        # break\n",
    "w.close()\n",
    "```\n",
    "\n",
    "将一些输出故障，暂时性缺省为不匹配。\n",
    "\n",
    "### 2.1.4 输出检查\n",
    "\n",
    "* 3.check.py\n",
    "\n",
    "```python\n",
    "import json\n",
    "import linecache\n",
    "\n",
    "\n",
    "def check(submit_path, test_path, max_num=15000):\n",
    "    '''\n",
    "    :param submit_path: 提交的文件名\n",
    "    :param test_path: 原始测试数据名\n",
    "    :param max_num: 测试数据大小\n",
    "    :return:\n",
    "    '''\n",
    "    N = 0\n",
    "    with open(submit_path, 'r', encoding='utf-8') as fs:\n",
    "        for line in fs:\n",
    "            try:\n",
    "                line = line.strip()\n",
    "                if line == '':\n",
    "                    continue\n",
    "                N += 1\n",
    "                smt_jsl = json.loads(line)\n",
    "                test_jsl = json.loads(linecache.getline(test_path, N))\n",
    "                if set(smt_jsl.keys()) != {'text_id', 'query', 'candidate'}:\n",
    "                    raise AssertionError(f'请保证提交的JSON数据的所有key与测评数据一致！ {line}')\n",
    "                elif smt_jsl['text_id'] != test_jsl['text_id']:\n",
    "                    raise AssertionError(f'请保证text_id和测评数据一致，并且不要改变数据顺序！text_id: {smt_jsl[\"text_id\"]}')\n",
    "                elif smt_jsl['query'] != test_jsl['query']:\n",
    "                    raise AssertionError(f'请保证query内容和测评数据一致！text_id: {smt_jsl[\"text_id\"]}, query: {smt_jsl[\"query\"]}')\n",
    "                elif len(smt_jsl['candidate']) != len(test_jsl['candidate']):\n",
    "                    raise AssertionError(f'请保证candidate的数量和测评数据一致！text_id: {smt_jsl[\"text_id\"]}')\n",
    "                else:\n",
    "                    for smt_cand, test_cand in zip(smt_jsl['candidate'], test_jsl['candidate']):\n",
    "                        if smt_cand['text'] != test_cand['text']:\n",
    "                            raise AssertionError(f'请保证candidate的内容和顺序与测评数据一致！text_id: {smt_jsl[\"text_id\"]}, candidate_item: {smt_cand[\"text\"]}')\n",
    "\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                raise AssertionError(f'请检查提交数据的JSON格式是否有误！如：用键-值用双引号 {line}')\n",
    "            except KeyError:\n",
    "                raise AssertionError(f'请保证提交的JSON数据的所有key与测评数据一致！{line}')\n",
    "\n",
    "    if N != max_num:\n",
    "        raise AssertionError(f\"请保证测试数据的完整性(共{max_num}条)，不可丢失或增加数据！\")\n",
    "\n",
    "    print('Well Done ！！')\n",
    "\n",
    "check('test_format.txt', 'x3nlp_1_test.txt')\n",
    "```\n",
    "保证数据条目、格式等等均正常，输出Well Done ！！即可！\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 模型细调\n",
    "\n",
    "参考 [https://github.com/PaddlePaddle/ERNIE/blob/release/v1.5/docs/erniekit.md](https://github.com/PaddlePaddle/ERNIE/blob/release/v1.5/docs/erniekit.md)\n",
    "\n",
    "### 2.2.1 数据准备\n",
    "\n",
    "参考 [https://github.com/PaddlePaddle/ERNIE/tree/release/v1.5/examples/data](https://github.com/PaddlePaddle/ERNIE/tree/release/v1.5/examples/data)\n",
    "\n",
    "这里我们的SFT Supervised Fine-tuning 采用的数据格式如下：\n",
    "\n",
    "```shell\n",
    "# https://github.com/PaddlePaddle/ERNIE/blob/release/v1.5/examples/data/sft-eval.jsonl\n",
    "{\"src\": [\"列出三首在90年代流行的歌曲。\"], \"tgt\": [\"\\\"I Want It That Way\\\"（唱：Backstreet Boys）、\\\"Torn\\\"（唱：Natalie Imbruglia）和\\\"Livin' La Vida Loca\\\"（唱：Ricky Martin）。\"]}\n",
    "{\"src\": [\"描述正确处理危险废物的方法。\"], \"tgt\": [\"危险废物应该被收集在密封容器中，并带到授权的危险废物处理设施。容器应标有内部废物的类型和其处理方法的说明。\"]}\n",
    "{\"src\": [\"写一个关于时间旅行者的创意故事。\"], \"tgt\": [\"从前，有一个叫蒂姆的年轻人，他热爱探索和体验新的地方和文化。他偶然得到了一只神奇的手表，可以让他回到过去。他利用这只手表穿越不同的时期，去探索他祖先生活过的世界。他的冒险带他穿越野西到古老的朝代，来回穿梭于时空之间，试图解决几个世纪以来无法解决的难题和谜题。\"]}\n",
    "{\"src\": [\"提供三个使用“redundant”一词的例句\"], \"tgt\": [\"1. 这份文件包含了很多不必要的冗余信息。\\n2. 系统更新后，一些冗余代码被删除了。\\n3. 有冗余的备份保证了数据不会永久丢失。\"]}\n",
    "{\"src\": [\"辨别以下短语是真是假：“宇宙正在不断扩张。”\"], \"tgt\": [\"真。\"]}\n",
    "{\"src\": [\"从短语“人工智能教育”中创建三个研究论文标题\"], \"tgt\": [\"1. 人工智能教育有效性的全面研究\\n2. 利用人工智能在教育中面临的挑战和机遇\\n3. 探索将人工智能作为增强教育成果的工具\"]}\n",
    "{\"src\": [\"给我一本受欢迎的儿童书的名称。\"], \"tgt\": [\"猫和帽子里的猫。\"]}\n",
    "{\"src\": [\"在南美洲中找出三个国家\"], \"tgt\": [\"巴西、哥伦比亚和阿根廷。\"]}\n",
    "{\"src\": [\"将华氏温度80度转换为相应的摄氏温度\"], \"tgt\": [\"26.7摄氏度。\"]}\n",
    "{\"src\": [\"写三句话描述一个宁静的海滩\"], \"tgt\": [\"1. 当柔和的海浪缓缓地涌入海滩时，太阳在清澈的蓝色海洋上闪闪发光。\\n2.沙滩上的白沙在轻微的微风中闪闪发光。\\n3.一群海鸥在海浪的伴随中高歌，并齐声吟唱。\"]}\n",
    "```\n",
    "根据这个格式，准备数据：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _name, _data in [\n",
    "    [\"SFT-train.jsonl\", d1],\n",
    "    [\"SFT-eval.jsonl\", d2],\n",
    "]:\n",
    "    with open(_name, \"w\") as f:\n",
    "        for i in _data[\"text\"]:\n",
    "            j = json.loads(i)\n",
    "            j1 = {\n",
    "                \"query\": j[\"query\"], \n",
    "                \"candidate\": [{\"text\": ij[\"text\"]} for ij in j[\"candidate\"]]\n",
    "            }\n",
    "            j2 = {\n",
    "                \"query\": j[\"query\"], \n",
    "                \"candidate\": j[\"candidate\"]\n",
    "            }\n",
    "            j3 = {\n",
    "                \"src\": [json.dumps(j1, ensure_ascii=False)],\n",
    "                \"tgt\": [json.dumps(j2, ensure_ascii=False)],\n",
    "            }\n",
    "            # print(j1, \"\\n\\n\", j2, \"\\n\\n\", j3)\n",
    "            f.write(json.dumps(j3, ensure_ascii=False)+\"\\n\")\n",
    "            # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 开始训练\n",
    "\n",
    "* 参数配置/SFT-run_sft_lora_8k.yaml\n",
    "\n",
    "```yaml\n",
    "### data\n",
    "train_dataset_type: \"erniekit\"\n",
    "eval_dataset_type: \"erniekit\"\n",
    "train_dataset_path: \"./SFT-train.jsonl\"\n",
    "train_dataset_prob: \"1.0\"\n",
    "eval_dataset_path: \"./SFT-eval.jsonl\"\n",
    "eval_dataset_prob: \"1.0\"\n",
    "max_seq_len: 8192\n",
    "num_samples_each_epoch: 6000000\n",
    "\n",
    "### model\n",
    "model_name_or_path: data/models/ERNIE-4.5-21B-A3B-Base-Paddle\n",
    "moe_group: mp\n",
    "fine_tuning: LoRA\n",
    "lora_rank: 32\n",
    "fuse_rope: True\n",
    "use_sparse_head_and_loss_fn: True\n",
    "\n",
    "### finetuning\n",
    "# base\n",
    "stage: SFT\n",
    "seed: 23\n",
    "do_train: True\n",
    "do_eval: True\n",
    "distributed_dataloader: False\n",
    "dataloader_num_workers: 1\n",
    "batch_size: 1\n",
    "num_train_epochs: 1\n",
    "max_steps: 600\n",
    "max_evaluate_steps: 10000\n",
    "eval_steps: 10000\n",
    "evaluation_strategy: steps\n",
    "save_steps: 10000000\n",
    "save_total_limit: 5\n",
    "save_strategy: steps\n",
    "logging_steps: 1\n",
    "release_grads: True\n",
    "gradient_accumulation_steps: 8\n",
    "logging_dir: ./vdl_log\n",
    "output_dir: data/models/ERNIE-4.5-21B-A3B-Base-LORA\n",
    "disable_tqdm: True\n",
    "\n",
    "# train\n",
    "warmup_steps: 20\n",
    "learning_rate: 3.0e-4\n",
    "lr_scheduler_type: cosine\n",
    "min_lr: 1.0e-6\n",
    "layerwise_lr_decay_bound: 1.0\n",
    "\n",
    "# optimizer\n",
    "weight_decay: 0.1\n",
    "adam_epsilon: 1.0e-8\n",
    "adam_beta1: 0.9\n",
    "adam_beta2: 0.95\n",
    "offload_optim: True\n",
    "\n",
    "# performance\n",
    "tensor_parallel_degree: 1\n",
    "pipeline_parallel_degree: 1\n",
    "sharding_parallel_degree: 1\n",
    "sharding: stage1\n",
    "sequence_parallel: True\n",
    "pipeline_parallel_config: disable_partial_send_recv enable_clear_every_step_cache\n",
    "recompute: True\n",
    "recompute_use_reentrant: True\n",
    "compute_type: bf16\n",
    "fp16_opt_level: O2\n",
    "disable_ckpt_quant: True\n",
    "amp_master_grad: True\n",
    "amp_custom_white_list:\n",
    "  - lookup_table\n",
    "  - lookup_table_v2\n",
    "  - flash_attn\n",
    "  - matmul\n",
    "  - matmul_v2\n",
    "  - fused_gemm_epilogue\n",
    "amp_custom_black_list:\n",
    "  - reduce_sum\n",
    "  - softmax_with_cross_entropy\n",
    "  - c_softmax_with_cross_entropy\n",
    "  - elementwise_div\n",
    "  - sin\n",
    "  - cos\n",
    "unified_checkpoint: True\n",
    "unified_checkpoint_config: async_save\n",
    "```\n",
    "\n",
    "主要配置：\n",
    "\n",
    "* SFT训练\n",
    "\n",
    "train_dataset_path: \"./SFT-train.jsonl\"\n",
    "\n",
    "* SFT验证\n",
    "\n",
    "eval_dataset_path: \"./SFT-eval.jsonl\"\n",
    "\n",
    "* 模型路径\n",
    "\n",
    "model_name_or_path: data/models/ERNIE-4.5-21B-A3B-Base-Paddle\n",
    "\n",
    "* 运行步数\n",
    "\n",
    "max_steps: 600\n",
    "\n",
    "* 输出路径\n",
    "\n",
    "output_dir: data/models/ERNIE-4.5-21B-A3B-Base-LORA\n",
    "\n",
    "然后，开始SFT：\n",
    "\n",
    "```python\n",
    "os.system(f\"rm -rf {p1}\")\n",
    "os.system(\"erniekit train SFT-run_sft_lora_8k.yaml\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 合并权重\n",
    "\n",
    "* 参数配置/SFT-run_export.yaml\n",
    "\n",
    "```yaml\n",
    "### model\n",
    "model_name_or_path: data/models/ERNIE-4.5-21B-A3B-Base-Paddle\n",
    "fine_tuning: LoRA\n",
    "\n",
    "### split\n",
    "max_shard_size: 5\n",
    "hf_hub_id: null\n",
    "output_dir: data/models/ERNIE-4.5-21B-A3B-Base-LORA\n",
    "\n",
    "### performance\n",
    "tensor_parallel_degree: 1\n",
    "pipeline_parallel_degree: 1\n",
    "sharding_parallel_degree: 1\n",
    "sharding: stage1\n",
    "pipeline_parallel_config: disable_partial_send_recv enable_clear_every_step_cache\n",
    "sequence_parallel: True\n",
    "compute_type: bf16\n",
    "fp16_opt_level: O2\n",
    "```\n",
    "\n",
    "主要配置：\n",
    "\n",
    "* 模型路径\n",
    "model_name_or_path: data/models/ERNIE-4.5-21B-A3B-Base-Paddle\n",
    "\n",
    "* 输出路径\n",
    "output_dir: data/models/ERNIE-4.5-21B-A3B-Base-LORA\n",
    "\n",
    "然后，开始合并：\n",
    "\n",
    "```python\n",
    "os.system(f\"rm -rf {p1}/export\")\n",
    "os.system(\"erniekit export SFT-run_export.yaml lora=True\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 重新部署\n",
    "\n",
    "参考 2.1.1 快速部署，修改模型路径。\n",
    "\n",
    "```python\n",
    "os.system(f\"python -m fastdeploy.entrypoints.openai.api_server --model {p1}/export --port 8180 --metrics-port 8181 --engine-worker-queue-port 8182 --max-model-len 1024 --max-num-seqs 128\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4.SFT.py\n",
    "\n",
    "```python\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.system(\"clear\")\n",
    "\n",
    "\n",
    "# 1.Data Preparation\n",
    "# CUT TRAIN && EVAL DATA TO JSONL\n",
    "data = pd.read_csv(\n",
    "    \"x3nlp_1_train.txt\", sep=\"\\t\", header=None, names=[\"text\"]\n",
    ")\n",
    "d1, d2 = train_test_split(data, test_size=0.2)\n",
    "print(d1.shape, d2.shape)\n",
    "\n",
    "for _name, _data in [\n",
    "    [\"SFT-train.jsonl\", d1],\n",
    "    [\"SFT-eval.jsonl\", d2],\n",
    "]:\n",
    "    with open(_name, \"w\") as f:\n",
    "        for i in _data[\"text\"]:\n",
    "            j = json.loads(i)\n",
    "            j1 = {\n",
    "                \"query\": j[\"query\"], \n",
    "                \"candidate\": [{\"text\": ij[\"text\"]} for ij in j[\"candidate\"]]\n",
    "            }\n",
    "            j2 = {\n",
    "                \"query\": j[\"query\"], \n",
    "                \"candidate\": j[\"candidate\"]\n",
    "            }\n",
    "            j3 = {\n",
    "                \"src\": [json.dumps(j1, ensure_ascii=False)],\n",
    "                \"tgt\": [json.dumps(j2, ensure_ascii=False)],\n",
    "            }\n",
    "            # print(j1, \"\\n\\n\", j2, \"\\n\\n\", j3)\n",
    "            f.write(json.dumps(j3, ensure_ascii=False)+\"\\n\")\n",
    "            # break\n",
    "\n",
    "\n",
    "p1 = \"data/models/ERNIE-4.5-21B-A3B-Base-LORA\"\n",
    "\n",
    "# 2.Supervised Fine-tuning\n",
    "os.system(f\"rm -rf {p1}\")\n",
    "os.system(\"erniekit train SFT-run_sft_lora_8k.yaml\")\n",
    "\n",
    "\n",
    "# 3.Weight Merging\n",
    "os.system(f\"rm -rf {p1}/export\")\n",
    "os.system(\"erniekit export SFT-run_export.yaml lora=True\")\n",
    "\n",
    "\n",
    "# 4.Load Model To Server\n",
    "os.system(f\"python -m fastdeploy.entrypoints.openai.api_server --model {p1}/export --port 8180 --metrics-port 8181 --engine-worker-queue-port 8182 --max-model-len 1024 --max-num-seqs 128\")\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、结果对比\n",
    "\n",
    "## 3.1 泛化能力\n",
    "\n",
    "### 3.1.1 模型A\n",
    "\n",
    "|Model|F1|Note|\n",
    "|-|-|-|\n",
    "|模型A|**0.2653**||\n",
    "\n",
    "模型A简单直接，但从F1分数上看，能力有点一般。\n",
    "\n",
    "### 3.1.2 模型B\n",
    "\n",
    "|Model|F1|Note|\n",
    "|-|-|-|\n",
    "|模型B|**0.3483**||\n",
    "\n",
    "事实上，确实模型B要比模型A更好，F1分数有0.08+提升。\n",
    "\n",
    "### 3.1.3 模型C\n",
    "\n",
    "|Model|F1|Note|\n",
    "|-|-|-|\n",
    "|模型C|**0.3696**|CROSS(A+B)|\n",
    "\n",
    "模型融合后的模型C，F1分数有0.02提升，融合模型A后稍有增益。\n",
    "\n",
    "### 3.1.4 ERNIE-4.5-0.3B-Base\n",
    "\n",
    "|Model|F1|Note|\n",
    "|-|-|-|\n",
    "|ERNIE-4.5-0.3B-Base|**0.3719**|*para|\n",
    "\n",
    "参数规模0.3B，基础预训练模型，极致轻量，推理速度快，比模型B、模型C稍好，能力相对持平。\n",
    "\n",
    "### 3.1.5 ERNIE-4.5-21B-A3B\n",
    "\n",
    "|Model|F1|Note|\n",
    "|-|-|-|\n",
    "|ERNIE-4.5-21B-A3B|**0.4843**|*para|\n",
    "\n",
    "参数规模21B，中大型任务，科研实验常用，F1分数比模型C有0.11+提升，开始降维打击！\n",
    "\n",
    "### 3.1.6 ERNIE-4.5-21B-A3B-Base\n",
    "\n",
    "|Model|F1|Note|\n",
    "|-|-|-|\n",
    "|ERNIE-4.5-21B-A3B-Base|**0.5537**|*para|\n",
    "\n",
    "参数规模21B，擅长通用文本生成，中型任务，F1分数比模型C有0.18+提升，直接拉开差距！强！\n",
    "\n",
    "## 3.2 模型细调\n",
    "\n",
    "### 3.2.1 ERNIE-4.5-21B-A3B-Base-LORA\n",
    "\n",
    "|Model|F1|Note|\n",
    "|-|-|-|\n",
    "|ERNIE-4.5-21B-A3B-Base|**0.7741**|*para && SFT-lora-8k-2w-S600-.8|\n",
    "\n",
    "基于相同的ERNIE-4.5-21B-A3B-Base，切取80%训练集作为SFT LORA 8K的模型细调，经过600steps之后，F1分数再有0.22+提升，进一步拉开差距！当然，我们也补充了其他形式的NLP模型进行训练，F1分数大概在0.52-0.82，细调后已具中上水平了。\n",
    "\n",
    "当然，更加大的参数规模和更加深刻的SFT，效果可能还会更好！这就留作后话吧！有兴趣的朋友们可以自行试试！\n",
    "\n",
    "特别说明，*para 即同一参数设置，详见章节二。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、策略展望\n",
    "\n",
    "随着自然语言处理引擎的升级，对上述反欺诈策略在不同业务阶段进行流程复盘，并推演策略全景。\n",
    "\n",
    "## 4.1 初级阶段\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/cb7d800606d440969200dff95c5480b9c3d76367e42444e0a606c74f46d6721a)\n",
    "\n",
    "主要依靠纯人工审批，或传统黑名单、专家经验为主的业务规则。当发现疑似个案后，通过各系统简单查询，发现由疑似个案牵连出来的欺诈群体，并根据人工判断是否实际欺诈，结合不同申请状态采用对应的反制举措。重度依赖审批专家经验判别，准确性较好，但人工成本过高，且案件排查未能充分挖掘出与疑似个案关联所有申请件。\n",
    "\n",
    "## 4.2 策略进阶\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/7cf62a0a24fe4505be6c962880117d2124e02b0f9c01425689f632b52779c1d9)\n",
    "\n",
    "地址匹配能力升级之后，类似于**模型A、模型B、模型C、ERNIE-4.5-0.3B-Base**。\n",
    "\n",
    "在初级阶段基础上，先后引入反欺诈模型、关联网络技术，强化数据验证、数据反馈机制，建立黑名单库。在关联网络+模糊匹配加持下，由欺诈个案将牵连出更完整的欺诈社团，并更具模型评分判断是否实际欺诈。反欺诈模型坏样本少，需通过贷前审批、贷后催收反馈收集，模型更易学会已知欺诈模式，而对未知模式可能束手无策，需要快速迭代。\n",
    "\n",
    "## 4.3 全景推演\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/077e76aa2c6a4b439d766f68f8f4c3e5b0eae43b32b84734a22c6fe06fd61a9b)\n",
    "\n",
    "如果能有更强的匹配技术，如：**ERNIE-4.5-21B-A3B、ERNIE-4.5-21B-A3B-Base、ERNIE-4.5-21B-A3B-Base-LORA**。\n",
    "\n",
    "据此推演更加全面的反欺诈策略流程：\n",
    "\n",
    "● 强化信息验证\n",
    "\n",
    "引入外部数据、埋点数据，并充分考虑查询成本、客户体验；\n",
    "\n",
    "● 获取潜在关系\n",
    "\n",
    "实时计算新进申请件所在关联网络结果，返回其欺诈网络、关联特征等；\n",
    "\n",
    "● 完善数据反馈\n",
    "\n",
    "打通贷前审批、案件调查、舆情监控、贷中异常、贷后催收等业务环节的数据反馈收集；\n",
    "\n",
    "● 识别欺诈模式\n",
    "\n",
    "不断数据积累，据此构建覆盖已知+未知欺诈模式的反欺诈模型，并返回欺诈评分，以及对应群体的电核问题、业务提示，降低电核成本；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 写在最后\n",
    "\n",
    "关于智能金融方向，有兴趣的朋友们可以查阅[智能金融之决策科学篇](https://aistudio.baidu.com/projectdetail/8623759)，希望可以帮到大家。\n",
    "\n",
    "更多的，欢迎关注[AI Studio](https://aistudio.baidu.com/personalcenter/thirdview/979775)/[GitHub](https://github.com/IvanaXu)。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
